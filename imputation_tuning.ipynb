{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88faf5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import dask\n",
    "from dask import delayed\n",
    "import logging\n",
    "from dask.distributed import LocalCluster, SSHCluster, Client\n",
    "import socket\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import sklearn\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9efaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for MissForest (it is only compatible up to scikit-learn 1.1.3!!)\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from missingpy import MissForest\n",
    "\n",
    "# needed for GAIN\\n\",\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from GAIN import gain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf21f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(x_in, y_in):\n",
    "    # NOT WORKING ON DASK DELAYED OBJECTS\n",
    "    # scale data\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler().fit(x_in)  # RobustScaler().fit(x_in)\n",
    "    x_scaled = pd.DataFrame(scaler.transform(x_in), columns=x_in.columns)\n",
    "    y_scaled = pd.DataFrame(scaler.transform(y_in), columns=y_in.columns)\n",
    "    return x_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_values_globally_and_scale(x_in, proportion_in, parameters_in):\n",
    "    # randomly remove k% from all parameter columns (globally) (100% equals the LENGTH of x, not length*width\n",
    "    # to keep it comparable to the other run (where k% are removed from one column only)\n",
    "    x = x_in.copy()  # we do not want to change inputs directly\n",
    "    x_rows = x[parameters_in].shape[0]\n",
    "    x_cols = x[parameters_in].shape[1]\n",
    "    # round(len(x) * proportion_in / 100)  # number of values to set to nan\n",
    "    num_vals = round(len(x)*len(parameters_in)*proportion_in/100)\n",
    "\n",
    "    nan_mat = np.zeros(shape=(x_rows * x_cols,), dtype=bool)  # create array of False\n",
    "    nan_mat[:num_vals] = True  # set first num_vals elements to True\n",
    "    nan_mat = np.random.permutation(nan_mat)  # shuffle\n",
    "    nan_mat = np.reshape(nan_mat, (x_rows, x_cols))  # bring to right shape\n",
    "\n",
    "    # concat lat, lon, depth columns with parameter columns (only from latter value were removed)\n",
    "    x = pd.concat([x[[p for p in x.columns if p not in parameters_in]], x[parameters_in].mask(nan_mat)], axis=1)\n",
    "\n",
    "    # scale data\n",
    "    x_scaled, y_scaled = scale_data(x, x_in)\n",
    "    return x_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_values_from_one_param_by_index_and_scale(x_in, parameter_in, indexes_in):\n",
    "    x = x_in.copy()\n",
    "    x.loc[indexes_in, parameter_in] = np.nan\n",
    "\n",
    "    # scale data\n",
    "    x_scaled, y_scaled = scale_data(x, x_in)\n",
    "    return x_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_complete_tuple_and_scale(x_in, indexes_in):\n",
    "    x = x_in.copy()\n",
    "    x.loc[indexes_in, :] = np.nan\n",
    "\n",
    "    # scale data\n",
    "    x_scaled, y_scaled = scale_data(x, x_in)\n",
    "    return x_scaled, y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imputer(x_in, y_in, predicting_in, hyperparamerter_dict_in, imputer_name_in):\n",
    "    if imputer_name_in == \"gain_imputer\":\n",
    "        s_impute_time = time.time()\n",
    "        x_hat = pd.DataFrame(gain.gain(x_in.to_numpy(), hyperparamerter_dict_in), columns=x_in.columns)\n",
    "        impute_time = time.time() - s_impute_time\n",
    "    else:\n",
    "        if imputer_name_in == \"knn_imputer\":\n",
    "            imputer = KNNImputer(**hyperparamerter_dict_in)\n",
    "        elif imputer_name_in == \"missforest_imputer\":\n",
    "            imputer = MissForest(**hyperparamerter_dict_in)\n",
    "        elif imputer_name_in == \"iterative_ridge_imputer\" or imputer_name_in == \"iterative_rf_imputer\":\n",
    "            imputer = IterativeImputer(**hyperparamerter_dict_in)\n",
    "        elif imputer_name_in == \"mean_imputer\":\n",
    "            imputer = SimpleImputer(**hyperparamerter_dict_in)\n",
    "            # x_hat = imputer.fit_transform(x_in)\n",
    "        else:\n",
    "            print(\"run_imputer: Unknown imputer!\")\n",
    "\n",
    "        s_impute_time = time.time()\n",
    "        x_hat = pd.DataFrame(imputer.fit_transform(x_in), columns=x_in.columns)  # @todo can we use pd or dd dataframes??\n",
    "        impute_time = time.time() - s_impute_time\n",
    "\n",
    "    # compute RMSE\n",
    "    rmse = np.linalg.norm(x_hat[predicting_in] - y_in[predicting_in]) / np.sqrt(len(y_in))\n",
    "\n",
    "    return rmse, impute_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c28b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def experiment0(x_in, missing_value_proportion_in, parameters_in, grid_search_in, param_grid_in, imputer_name_in,\n",
    "                iteration_in, fold_in):\n",
    "    x0, y0 = drop_values_globally_and_scale(x_in=x_in,\n",
    "                                            proportion_in=missing_value_proportion_in,\n",
    "                                            parameters_in=parameters_in)\n",
    "\n",
    "    res = []\n",
    "    for hyperparamerter_combination in grid_search_in:\n",
    "        hyperparams_dict = dict((b, a) for a, b in zip(hyperparamerter_combination, param_grid_in.keys()))\n",
    "        rmse0, impute_time0 = run_imputer(x_in=x0, y_in=y0,\n",
    "                                          predicting_in=parameters_in,\n",
    "                                          hyperparamerter_dict_in=hyperparams_dict,\n",
    "                                          imputer_name_in=imputer_name_in)\n",
    "        res.append(pd.DataFrame({\"iteration\": [iteration_in],\n",
    "                                 \"missing_value_proportion\": [missing_value_proportion_in],\n",
    "                                 \"predicting\": [str(parameters_in)],\n",
    "                                 \"imputer\": [imputer_name_in],\n",
    "                                 \"time\": [impute_time0],\n",
    "                                 \"rmse\": [rmse0],\n",
    "                                 \"hyperparameters\": [hyperparams_dict],\n",
    "                                 \"fold\": [fold_in]}))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def experiment1(x_in, indexes_to_remove_in, missing_value_proportion_in, parameters_in, grid_search_in, param_grid_in,\n",
    "                imputer_name_in, iteration_in, fold_in):\n",
    "    res = []\n",
    "    for param in parameters_in:\n",
    "        x1, y1 = drop_values_from_one_param_by_index_and_scale(x_in=x_in, parameter_in=param,\n",
    "                                                               indexes_in=indexes_to_remove_in)\n",
    "        for hyperparamerter_combination in grid_search_in:\n",
    "            hyperparams_dict = dict((b, a) for a, b in zip(hyperparamerter_combination, param_grid_in.keys()))\n",
    "            rmse1, impute_time1 = run_imputer(x_in=x1, y_in=y1, predicting_in=param,\n",
    "                                              hyperparamerter_dict_in=hyperparams_dict, imputer_name_in=imputer_name_in)\n",
    "            res.append(pd.DataFrame({\"iteration\": [iteration_in],\n",
    "                                     \"missing_value_proportion\": [missing_value_proportion_in],\n",
    "                                     \"predicting\": [param],\n",
    "                                     \"imputer\": [imputer_name_in],\n",
    "                                     \"time\": [impute_time1],\n",
    "                                     \"rmse\": [rmse1],\n",
    "                                     \"hyperparameters\": [hyperparams_dict],\n",
    "                                     \"fold\": [fold_in]}))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def experiment2(x_in, indexes_to_remove_in, missing_value_proportion_in, parameters_in, grid_search_in, param_grid_in,\n",
    "                imputer_name_in, iteration_in, fold_in):\n",
    "    res = []\n",
    "    for param in parameters_in:\n",
    "        x2, y2 = drop_complete_tuple_and_scale(x_in=x_in, indexes_in=indexes_to_remove_in)\n",
    "        for hyperparamerter_combination in grid_search_in:\n",
    "            hyperparams_dict = dict((b, a) for a, b in zip(hyperparamerter_combination, param_grid_in.keys()))\n",
    "            rmse2, impute_time2 = run_imputer(x_in=x2, y_in=y2, predicting_in=param,\n",
    "                                              hyperparamerter_dict_in=hyperparams_dict, imputer_name_in=imputer_name_in)\n",
    "            res.append(pd.DataFrame({\"iteration\": [iteration_in],\n",
    "                                     \"missing_value_proportion\": [missing_value_proportion_in],\n",
    "                                     \"predicting\": [param],\n",
    "                                     \"imputer\": [imputer_name_in],\n",
    "                                     \"time\": [impute_time2],\n",
    "                                     \"rmse\": [rmse2],\n",
    "                                     \"hyperparameters\": [hyperparams_dict],\n",
    "                                     \"fold\": [fold_in]}))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec746d2b",
   "metadata": {},
   "source": [
    "# Tuning imputation models\n",
    "This notebook executes experiments to tune the hyperparameters of the imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad811b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output specification\n",
    "timestamp = round(time.time())\n",
    "output_path = f\"output/{timestamp}/\"\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb742d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_complete = pd.read_csv(\"../data/wide_table.csv\")\n",
    "parameters = list(filter(lambda x: x.startswith('P_'), list(df_complete.columns)))\n",
    "df = df_complete[[\"LATITUDE\", \"LONGITUDE\", \"LEV_M\"] + parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb1c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter combinations to test\n",
    "test_dict = [{\"imputer_name\": \"knn_imputer\",\n",
    "              \"param_grid\": {\"n_neighbors\": [1, 5, 10, 20, 30, 40, 50], \"weights\": [\"uniform\", \"distance\"]}},\n",
    "             {\"imputer_name\": \"missforest_imputer\",\n",
    "              \"param_grid\": {\"n_estimators\": [10, 50, 100, 200], \"max_iter\": [30], \"decreasing\": [False],\n",
    "                             \"criterion\": [\"squared_error\"], \"max_features\": [None], \"random_state\": [0],\n",
    "                             \"missing_values\": np.nan}},\n",
    "             {\"imputer_name\": \"gain_imputer\",\n",
    "              \"param_grid\": {\"alpha\": [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300], \"batch_size\": [128],\n",
    "                             \"hint_rate\": [0.9, 0.5], \"iterations\": [10000]}},\n",
    "             {\"imputer_name\": \"iterative_ridge_imputer\",\n",
    "              \"param_grid\": {\"tol\": [1e-3], \"max_iter\": [100, 1000], \"sample_posterior\": [False],\n",
    "                             \"estimator\": [BayesianRidge()], \"random_state\": [0]}},\n",
    "             {\"imputer_name\": \"mean_imputer\",\n",
    "              \"param_grid\": {\"missing_values\": [np.nan], \"strategy\": ['mean']}},\n",
    "             {\"imputer_name\": \"iterative_rf_imputer\",\n",
    "              \"param_grid\": {\"estimator\":  [RandomForestRegressor(n_estimators=5),\n",
    "                                            RandomForestRegressor(n_estimators=10)], \"max_iter\":[30],\n",
    "                             \"random_state\": [0]}}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae110b32",
   "metadata": {},
   "source": [
    "## Set up dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger(\"distributed.worker\")\n",
    "logging.basicConfig(filename='imputation_tuning_clean.log', level=logging.DEBUG)  # , encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a076f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70e066",
   "metadata": {},
   "source": [
    "## Set up experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69025103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for experiments\n",
    "imputer_name = \"knn_imputer\"  # mean_imputer, knn_imputer, missforest_imputer, gain_imputer, iterative_ridge_imputer, iterative_rf_imputer\n",
    "num_iterations = 50\n",
    "missing_value_proportions = [100 / len(df_train), 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b717ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid search of the hyperparameters\n",
    "param_grid = [x for x in test_dict if x[\"imputer_name\"] == imputer_name][0][\"param_grid\"]\n",
    "grid_search = list(it.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of runs\n",
    "num_runs_exp0 = num_iterations * len(missing_value_proportions) * len(grid_search)\n",
    "num_runs_exp1 = num_iterations * len(missing_value_proportions) * len(parameters) * len(grid_search)\n",
    "num_runs_exp2 = num_iterations * len(missing_value_proportions) * len(parameters) * len(grid_search)\n",
    "num_sum = num_runs_exp0 + num_runs_exp1 + num_runs_exp2\n",
    "print(f\"Number of runs: \\nExp0: {num_runs_exp0}\\nExp1: {num_runs_exp1}\\nExp2: {num_runs_exp2}\\nSum: {num_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result containers\n",
    "res0 = []\n",
    "res1 = []\n",
    "res2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942472e0",
   "metadata": {},
   "source": [
    "## Conduct experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef702f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "\n",
    "# run experiments\n",
    "with Client(cluster) as client:\n",
    "    print(client)\n",
    "    print(f\"Dashboard address: {client.dashboard_link}\")\n",
    "\n",
    "    # cross validation over folds\n",
    "    for fold in range(folds):\n",
    "        # load data\n",
    "        df_train = pd.read_csv(f\"data/train_table_{train_fraction}_fold{fold}.csv\")\n",
    "        df_test = pd.read_csv(f\"data/test_table_{train_fraction}_fold{fold}.csv\")\n",
    "        parameters = list(filter(lambda x: x.startswith('P_'), list(df_train.columns)))\n",
    "        \n",
    "        # experiment loop\n",
    "        for i in range(num_iterations):\n",
    "            for missing_value_proportion in missing_value_proportions:\n",
    "                # experiment 0\n",
    "                res0.append(experiment0(x_in=df_train,\n",
    "                                        missing_value_proportion_in=missing_value_proportion,\n",
    "                                        parameters_in=parameters,\n",
    "                                        grid_search_in=grid_search,\n",
    "                                        param_grid_in=param_grid,\n",
    "                                        imputer_name_in=imputer_name,\n",
    "                                        iteration_in=i,\n",
    "                                        fold_in=fold))\n",
    "\n",
    "                # experiments 1 and 2\n",
    "                num_indexes = np.round(len(df_train.index) / 100 * missing_value_proportion).astype(int)\n",
    "                indexes_to_remove = np.random.randint(0, len(df_train), num_indexes)\n",
    "                res1.append(experiment1(x_in=df_train,\n",
    "                                        indexes_to_remove_in=indexes_to_remove,\n",
    "                                        missing_value_proportion_in=missing_value_proportion,\n",
    "                                        parameters_in=parameters,\n",
    "                                        grid_search_in=grid_search,\n",
    "                                        param_grid_in=param_grid,\n",
    "                                        imputer_name_in=imputer_name,\n",
    "                                        iteration_in=i,\n",
    "                                        fold_in=fold))\n",
    "                # res2.append(experiment2(x_in=df_train,\n",
    "                #                         indexes_to_remove_in=indexes_to_remove,\n",
    "                #                         missing_value_proportion_in=missing_value_proportion,\n",
    "                #                         parameters_in=parameters,\n",
    "                #                         grid_search_in=grid_search,\n",
    "                #                         param_grid_in=param_grid,\n",
    "                #                         imputer_name_in=imputer_name,\n",
    "                #                         iteration_in=i,\n",
    "                #                         fold_in=fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph set up. Now computing.\")\n",
    "# compute and store output\n",
    "prefix = f\"{output_path}{timestamp}_hyperparameter_tuning_{imputer_name}\"\n",
    "tuning_filenames = []\n",
    "if res0:\n",
    "    temp0 = dask.compute(*res0)\n",
    "    df_res0 = pd.concat(list(itertools.chain.from_iterable(temp0)))\n",
    "    filename0 = f\"{prefix}0.csv\"\n",
    "    df_res0.to_csv(filename0, index=False)\n",
    "    tuning_filenames.append(filename0)\n",
    "if res1:\n",
    "    temp1 = dask.compute(*res1)\n",
    "    df_res1 = pd.concat(list(itertools.chain.from_iterable(temp1)))\n",
    "    filename1 = f\"{prefix}1.csv\"\n",
    "    df_res1.to_csv(filename1, index=False)\n",
    "    tuning_filenames.append(filename1)\n",
    "if res2:\n",
    "    temp2 = dask.compute(*res2)\n",
    "    df_res2 = pd.concat(list(itertools.chain.from_iterable(temp2)))\n",
    "    filename2 = f\"{prefix}2.csv\"\n",
    "    df_res2.to_csv(filename2, index=False)\n",
    "    tuning_filenames.append(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18684965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store run configuration\n",
    "overall_runtime = time.time() - start_time\n",
    "print(f\"Timestamp: {timestamp}\\nDuration: {round(overall_runtime, 2)} s\")\n",
    "df_run_config = pd.DataFrame({\"timestamp\": [timestamp], \"host\": [socket.gethostname()],\n",
    "                              \"output_path\": [output_path], \"overall_runtime\": [overall_runtime],\n",
    "                              \"parameters\": str(parameters),\n",
    "                              \"imputer_name\": [imputer_name],\n",
    "                              \"tuning_filenames\": [tuning_filenames], \"tuning_iterations\": [num_iterations],\n",
    "                              \"tuning_missing_value_proportions\": str(missing_value_proportions),\n",
    "                              \"tuning_hyperparameters\": str(param_grid)})\n",
    "\n",
    "df_run_config.to_csv(f\"{output_path}config.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close dask infrastructure\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d028d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
