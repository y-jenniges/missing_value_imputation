{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bebd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yvjennig\\PycharmProjects\\phd_repos\\clustering\n"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "import os\n",
    "os.chdir(\"C:/Users/yvjennig/PycharmProjects/phd_repos/clustering/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "import itertools as it\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563efdb5",
   "metadata": {},
   "source": [
    "# Training on complete training dataset\n",
    "\n",
    "Now that we know the optimal combination of hyperparameters (for each setting of missing_value_proportion, imputer, predicting), we can train a model for each combination. This is now done on the complete training set.\n",
    "\n",
    "We need experiments 0 and 1 for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce283c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result files\n",
    "scaler = \"minmaxscaler\"\n",
    "output_path = f\"output/{scaler}/\"\n",
    "model_path = f\"{output_path}/models/\"\n",
    "\n",
    "df_config = pd.read_csv(f\"{output_path}/train_results/config.csv\")\n",
    "df = pd.read_csv(f\"{output_path}/train_results/tuning_results.csv\")\n",
    "dfres = pd.read_csv(f\"{output_path}/train_results/optimal_hyperparameters.csv\")\n",
    "\n",
    "train_defaults = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1a3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir for models\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb9d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df_test = pd.read_csv(\"data/test_table_0.8.csv\")\n",
    "df_train = pd.read_csv(\"data/train_table_0.8.csv\")\n",
    "\n",
    "# define, fit and store scaler\n",
    "if scaler == \"robustscaler\": \n",
    "    scaler = RobustScaler().fit(df_train)\n",
    "elif scaler == \"minmaxscaler\": \n",
    "    scaler = MinMaxScaler().fit(df_train)\n",
    "else: \n",
    "    print(\"Unknown scaler!\")\n",
    "\n",
    "# store scaler model\n",
    "scaler_filename = f\"{model_path}scaler.pickle\"\n",
    "pickle.dump(scaler, open(scaler_filename, 'wb')) \n",
    "\n",
    "# scale data\n",
    "df_test_scaled = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train), columns=df_train.columns)\n",
    "\n",
    "# get parameters\n",
    "parameters = list(filter(lambda x: x.startswith('P_'), list(df_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6430cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all hyperparameter combinations to train per imputer\n",
    "hyperparameter_combinations = {}\n",
    "\n",
    "for imputer in dfres[\"imputer\"].unique():\n",
    "    temp = dfres[dfres[\"imputer\"] == imputer]\n",
    "\n",
    "    hyps = list(set(it.chain(temp[\"hyperparameters\"].unique())))  # make sure that each combo exists only once\n",
    "    df_hyper = pd.DataFrame(hyps, columns=[\"hyperparameters\"])  # put combinations into a dataframe (they are strings!)\n",
    "    dicts = df_hyper[\"hyperparameters\"].apply(eval)  # transform strings to dicts\n",
    "    hyperparameter_combinations[imputer] = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fcbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_missforest(hyps):\n",
    "    import sklearn.neighbors._base\n",
    "    sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "    from missingpy import MissForest\n",
    "\n",
    "    return MissForest(**hyps)  \n",
    "\n",
    "def run_gain(hyps):\n",
    "#     # needed for GAIN\n",
    "#     import tensorflow as tf\n",
    "#     tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#     from GAIN import gain\n",
    "#     x_hat = pd.DataFrame(gain.gain(df_train_scaled.to_numpy(), hyperparameter_combi), columns=df_train_scaled.columns)\n",
    "#     return x_hat\n",
    "    return \n",
    "\n",
    "def run_knn(hyps):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    return KNNImputer(**hyps)\n",
    "\n",
    "def run_mean(hyps):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    return SimpleImputer(**hyps)\n",
    "\n",
    "def run_iterativeRidge(hyps):\n",
    "    return IterativeImputer(**hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5a09ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_imputer\n",
      "    Training took: 0.0019829273223876953 s\n",
      "    Training took: 0.002008199691772461 s\n",
      "    Training took: 0.0010120868682861328 s\n",
      "    Training took: 0.001999378204345703 s\n",
      "    Training took: 0.002008676528930664 s\n",
      "    Training took: 0.0025267601013183594 s\n",
      "    Training took: 0.0020067691802978516 s\n",
      "mean_imputer\n",
      "    Training took: 0.0020093917846679688 s\n",
      "missforest\n",
      "    Training took: 0.004007101058959961 s\n",
      "    Training took: 0.003000020980834961 s\n",
      "    Training took: 0.003996610641479492 s\n",
      "    Training took: 0.003000020980834961 s\n",
      "    Training took: 0.0019998550415039062 s\n",
      "    Training took: 0.0030088424682617188 s\n",
      "    Training took: 0.003007173538208008 s\n",
      "    Training took: 0.004006862640380859 s\n",
      "gain_imputer\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0010001659393310547 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n",
      "    Training took: 0.0 s\n"
     ]
    }
   ],
   "source": [
    "# train one model per hyperparameter combination\n",
    "for imputer_name, hyperparameter_combis in hyperparameter_combinations.items():\n",
    "    print(imputer_name)\n",
    "    for hyperparameter_combi in hyperparameter_combis:\n",
    "        print(hyperparameter_combi)\n",
    "        s_impute_time = time.time()\n",
    "\n",
    "        if imputer_name == \"gain_imputer\":\n",
    "            x_hat = run_gain(hyps)\n",
    "        else:\n",
    "            if imputer_name == \"knn_imputer\":\n",
    "                imputer = run_knn(hyperparameter_combi)\n",
    "            elif imputer_name == \"missforest_imputer\" or imputer_name == \"missforest\":\n",
    "                imputer = run_missforest(hyperparameter_combi)\n",
    "            elif imputer_name == \"iterative_ridge\":\n",
    "                imputer = run_iterativeRidge(hyperparameter_combi)\n",
    "            elif imputer_name == \"mean_imputer\": \n",
    "                imputer = run_mean(hyperparameter_combi)\n",
    "\n",
    "            imputer.fit(df_train_scaled)\n",
    "\n",
    "        print(f\"    Training took: {time.time() - s_impute_time} s\")\n",
    "\n",
    "        filename = f\"{model_path}/{imputer_name}_{'-'.join([str(k) + '=' + str(v) for k, v in hyperparameter_combi.items()])}.pickle\"\n",
    "        pickle.dump(imputer, open(filename, 'wb'))  # store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e87d002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNImputer()\n",
      "SimpleImputer()\n",
      "MissForest()\n"
     ]
    }
   ],
   "source": [
    "# train default models\n",
    "for imputer_name in dfres[\"imputer\"].unique():\n",
    "    if train_defaults and imputer_name != \"gain_imputer\":\n",
    "        if imputer_name == \"knn_imputer\":\n",
    "            imputer = run_knn({})\n",
    "        elif imputer_name == \"missforest_imputer\" or imputer_name==\"missforest\":\n",
    "            imputer = run_missforest({})  \n",
    "        elif imputer_name == \"iterative_ridge\":\n",
    "            imputer = run_iterative_ridge({})\n",
    "        elif imputer_name == \"mean_imputer\": \n",
    "            imputer = run_mean({\"strategy\": \"mean\"})\n",
    "\n",
    "        imputer.fit(df_train_scaled)\n",
    "        print(imputer)\n",
    "\n",
    "        filename = f\"{model_path}{imputer_name}_default.pickle\"\n",
    "        pickle.dump(imputer, open(filename, 'wb'))  # store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eef388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4878e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641bec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f5967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74719fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b28b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d48f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
